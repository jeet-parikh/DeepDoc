{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632eff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain_community pymupdf openai faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e33e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = \"./S&DS 230 Final Report (1).pdf\"\n",
    "query = \"What is this report about?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84759fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "def parse_pdf(file_path):\n",
    "    \"\"\"Outputs the text documents on a parsed pdf\"\"\"\n",
    "\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    return loader.load()\n",
    "\n",
    "# my_file = \"./Resume - Parikh, Jeet.docx.pdf\"\n",
    "\n",
    "docs = parse_pdf(file_path=my_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f8121d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000, # should be larger later -> ~1000\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for doc in docs:\n",
    "    chunks += splitter.split_text(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "775ebf15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e25fa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "docs = [Document(page_content=chunk) for chunk in chunks]\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "faiss_index = FAISS.from_documents(docs, embedding_model)\n",
    "\n",
    "faiss_index.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f077fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "rag_prompt_template = \"\"\"\n",
    "You are a helpful assistant. Use the following context to answer the question. \n",
    "If the context does not contain the answer, say you don't know â€” do not make anything up.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], \n",
    "    template=rag_prompt_template\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name = \"gpt-4\",\n",
    "    temperature = 0.5\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c201d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The report discusses the use of statistical tools and knowledge to examine the patterns and predictors of life expectancy across the world. It covers the process of data cleaning, exploratory graphics, basic statistical tests, correlation exploration, multiple regression, ANOVA, ANCOVA, and logistic regression. It also talks about the selection of variables for the statistical analysis and the handling of missing values in the data.\n"
     ]
    }
   ],
   "source": [
    "loaded_faiss = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "results = loaded_faiss.similarity_search(query=query, k=4)\n",
    "retrieved_chunks = \"\\n\\n\".join([res.page_content for res in results])\n",
    "\n",
    "output = chain.run(context=retrieved_chunks, question=query)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
